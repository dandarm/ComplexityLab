  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "sigma = 10.0\n",
    "beta = 8.0 / 3.0\n",
    "r_values = [10, 15, 20, 24, 26, 28, 35]\n",
    "dt = 0.01\n",
    "transient = 2000\n",
    "steps = 6000\n",
    "\n",
    "\n",
    "def _lorenz_step(state, r):\n",
    "    x, y, z = state\n",
    "    dx = sigma * (y - x)\n",
    "    dy = x * (r - z) - y\n",
    "    dz = x * y - beta * z\n",
    "    return np.array([dx, dy, dz], dtype=float)\n",
    "\n",
    "\n",
    "def _rk4_step(state, r, dt):\n",
    "    k1 = _lorenz_step(state, r)\n",
    "    k2 = _lorenz_step(state + 0.5 * dt * k1, r)\n",
    "    k3 = _lorenz_step(state + 0.5 * dt * k2, r)\n",
    "    k4 = _lorenz_step(state + dt * k3, r)\n",
    "    return state + (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n",
    "\n",
    "\n",
    "lorenz_series = {}\n",
    "for r in r_values:\n",
    "    trajectory = np.zeros((transient + steps, 3), dtype=float)\n",
    "    trajectory[0] = np.array([1.0, 1.0, 1.0], dtype=float)\n",
    "    for i in range(transient + steps - 1):\n",
    "        trajectory[i + 1] = _rk4_step(trajectory[i], r, dt)\n",
    "    x_series = trajectory[transient:, 0]\n",
    "    x_series = (x_series - np.mean(x_series)) / np.std(x_series)\n",
    "    lorenz_series[r] = x_series\n",
    "\n",
    "noise_series = np.random.normal(0.0, 1.0, steps)\n",
    "noise_series = (noise_series - np.mean(noise_series)) / np.std(noise_series)\n",
    "\n",
    "all_series = {f\"r={r}\": series for r, series in lorenz_series.items()}\n",
    "all_series[\"rumore bianco\"] = noise_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esponenti di Lyapunov\n",
    "\n",
    "Calcoliamo l'esponente di Lyapunov massimo (\\(\\lambda_1\\)) mediante il metodo di Rosenstein su ricostruzioni a ritardo, confrontando i vari regimi del parametro \\(r\\) con un rumore bianco normalizzato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "def time_delay_embedding(series, emb_dim, delay):\n",
    "    series = np.asarray(series, dtype=float)\n",
    "    n_vectors = len(series) - (emb_dim - 1) * delay\n",
    "    if n_vectors <= 0:\n",
    "        raise ValueError(\"Serie troppo corta per l'embedding richiesto\")\n",
    "    return np.array([\n",
    "        series[i : i + emb_dim * delay : delay] for i in range(n_vectors)\n",
    "    ])\n",
    "\n",
    "\n",
    "def largest_lyapunov_rosenstein(series, dt, emb_dim=6, delay=8, theiler=50, fit_range=(5, 25)):\n",
    "    embedded = time_delay_embedding(series, emb_dim, delay)\n",
    "    tree = cKDTree(embedded)\n",
    "    distances, indices = tree.query(embedded, k=emb_dim + 2)\n",
    "    nn_indices = np.full(len(embedded), -1, dtype=int)\n",
    "\n",
    "    for i in range(len(embedded)):\n",
    "        for neighbor in indices[i, 1:]:\n",
    "            if neighbor == i:\n",
    "                continue\n",
    "            if abs(neighbor - i) > theiler:\n",
    "                nn_indices[i] = neighbor\n",
    "                break\n",
    "        if nn_indices[i] == -1:\n",
    "            nn_indices[i] = indices[i, 1]\n",
    "\n",
    "    max_t = min(200, len(embedded))\n",
    "    log_divergence = []\n",
    "    valid_steps = []\n",
    "    for t in range(max_t):\n",
    "        separations = []\n",
    "        for idx, neighbor in enumerate(nn_indices):\n",
    "            if neighbor < 0:\n",
    "                continue\n",
    "            if idx + t >= len(embedded) or neighbor + t >= len(embedded):\n",
    "                continue\n",
    "            dist = np.linalg.norm(embedded[idx + t] - embedded[neighbor + t])\n",
    "            if dist > 0:\n",
    "                separations.append(np.log(dist))\n",
    "        if separations:\n",
    "            log_divergence.append(np.mean(separations))\n",
    "            valid_steps.append(t * dt)\n",
    "        elif log_divergence:\n",
    "            break\n",
    "\n",
    "    if len(valid_steps) < 6:\n",
    "        return np.nan\n",
    "\n",
    "    start, end = fit_range\n",
    "    end = min(end, len(valid_steps))\n",
    "    start = min(start, end - 2)\n",
    "    coeffs = np.polyfit(valid_steps[start:end], log_divergence[start:end], 1)\n",
    "    return coeffs[0]\n",
    "\n",
    "\n",
    "lyapunov_rows = []\n",
    "for label, series in all_series.items():\n",
    "    try:\n",
    "        exponent = largest_lyapunov_rosenstein(series, dt)\n",
    "    except Exception:\n",
    "        exponent = np.nan\n",
    "    lyapunov_rows.append({\"Regime\": label, \"lambda1\": exponent})\n",
    "\n",
    "lyapunov_df = pd.DataFrame(lyapunov_rows).set_index(\"Regime\")\n",
    "lyapunov_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test di non linearitÃ  IAAFT\n",
    "\n",
    "Generiamo surrogati IAAFT che preservano distribuzione e spettro di potenza delle serie originali e utilizziamo \\(\\lambda_1\\) come statistica di test confrontando il valore reale con la distribuzione dei surrogati.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iaaft_surrogate(series, n_iterations=100):\n",
    "    series = np.asarray(series, dtype=float)\n",
    "    sorted_series = np.sort(series)\n",
    "    target_amplitude = np.abs(np.fft.rfft(series))\n",
    "    surrogate = np.random.permutation(series)\n",
    "    for _ in range(n_iterations):\n",
    "        surrogate_fft = np.fft.rfft(surrogate)\n",
    "        surrogate = np.fft.irfft(\n",
    "            target_amplitude * np.exp(1j * np.angle(surrogate_fft)), n=len(series)\n",
    "        )\n",
    "        ranks = np.argsort(np.argsort(surrogate))\n",
    "        surrogate = sorted_series[ranks]\n",
    "    return surrogate\n",
    "\n",
    "\n",
    "def iaaft_nonlinearity_test(series, n_surrogates=20):\n",
    "    reduced = series[::2]\n",
    "    statistics = []\n",
    "    for _ in range(n_surrogates):\n",
    "        surrogate = iaaft_surrogate(reduced, n_iterations=50)\n",
    "        statistics.append(largest_lyapunov_rosenstein(surrogate, dt))\n",
    "    statistics = np.array([s for s in statistics if np.isfinite(s)])\n",
    "    observed = largest_lyapunov_rosenstein(reduced, dt)\n",
    "    if statistics.size == 0 or not np.isfinite(observed):\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    mean = statistics.mean()\n",
    "    std = statistics.std(ddof=1) if statistics.size > 1 else np.nan\n",
    "    z_score = (observed - mean) / std if np.isfinite(std) and std > 0 else np.nan\n",
    "    p_value = (np.sum(statistics >= observed) + 1) / (statistics.size + 1)\n",
    "    return observed, mean, std, z_score, p_value\n",
    "\n",
    "\n",
    "iAAFT_rows = []\n",
    "for label, series in all_series.items():\n",
    "    if label == \"rumore bianco\":\n",
    "        continue\n",
    "    obs, mean, std, z, p = iaaft_nonlinearity_test(series)\n",
    "    iAAFT_rows.append(\n",
    "        {\n",
    "            \"Regime\": label,\n",
    "            \"lambda1_obs\": obs,\n",
    "            \"lambda1_surrogati\": mean,\n",
    "            \"std_surrogati\": std,\n",
    "            \"z_score\": z,\n",
    "            \"p_value\": p,\n",
    "        }\n",
    "    )\n",
    "\n",
    "iaaft_df = pd.DataFrame(iAAFT_rows).set_index(\"Regime\")\n",
    "iaaft_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hurst exponent e DFA\n",
    "\n",
    "Stimiamo l'esponente di Hurst tramite il metodo R/S e l'esponente \\(\\alpha\\) tramite DFA (Detrended Fluctuation Analysis) per valutare la persistenza nelle serie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hurst_rs(series, min_window=16, max_window=None, num_windows=20):\n",
    "    series = np.asarray(series, dtype=float)\n",
    "    series = series - np.mean(series)\n",
    "    n = len(series)\n",
    "    if max_window is None:\n",
    "        max_window = n // 6\n",
    "    windows = np.unique(\n",
    "        np.logspace(\n",
    "            np.log10(min_window), np.log10(max_window), num=num_windows, dtype=int\n",
    "        )\n",
    "    )\n",
    "    rs_values = []\n",
    "    valid_windows = []\n",
    "    for w in windows:\n",
    "        if w < min_window or w >= n // 2:\n",
    "            continue\n",
    "        n_segments = n // w\n",
    "        if n_segments < 2:\n",
    "            continue\n",
    "        data = series[: n_segments * w].reshape((n_segments, w))\n",
    "        data = data - data.mean(axis=1, keepdims=True)\n",
    "        cumulative = np.cumsum(data, axis=1)\n",
    "        ranges = cumulative.max(axis=1) - cumulative.min(axis=1)\n",
    "        stds = data.std(axis=1, ddof=1)\n",
    "        valid = stds > 0\n",
    "        if not np.any(valid):\n",
    "            continue\n",
    "        rs = np.mean(ranges[valid] / stds[valid])\n",
    "        rs_values.append(rs)\n",
    "        valid_windows.append(w)\n",
    "    if len(rs_values) < 2:\n",
    "        return np.nan\n",
    "    slope, _ = np.polyfit(np.log(valid_windows), np.log(rs_values), 1)\n",
    "    return slope\n",
    "\n",
    "\n",
    "def dfa_alpha(series, min_window=16, max_window=None, num_windows=20, order=1):\n",
    "    series = np.asarray(series, dtype=float)\n",
    "    series = series - np.mean(series)\n",
    "    n = len(series)\n",
    "    if max_window is None:\n",
    "        max_window = n // 6\n",
    "    windows = np.unique(\n",
    "        np.logspace(\n",
    "            np.log10(min_window), np.log10(max_window), num=num_windows, dtype=int\n",
    "        )\n",
    "    )\n",
    "    profile = np.cumsum(series)\n",
    "    flucts = []\n",
    "    valid_windows = []\n",
    "    for w in windows:\n",
    "        if w < min_window or w >= n // 2:\n",
    "            continue\n",
    "        n_segments = n // w\n",
    "        if n_segments < 2:\n",
    "            continue\n",
    "        segments = profile[: n_segments * w].reshape((n_segments, w))\n",
    "        x = np.arange(w)\n",
    "        rms = []\n",
    "        for segment in segments:\n",
    "            coeffs = np.polyfit(x, segment, order)\n",
    "            trend = np.polyval(coeffs, x)\n",
    "            rms.append(np.sqrt(np.mean((segment - trend) ** 2)))\n",
    "        rms = np.array(rms)\n",
    "        if np.all(rms == 0):\n",
    "            continue\n",
    "        flucts.append(np.mean(rms))\n",
    "        valid_windows.append(w)\n",
    "    if len(flucts) < 2:\n",
    "        return np.nan\n",
    "    slope, _ = np.polyfit(np.log(valid_windows), np.log(flucts), 1)\n",
    "    return slope\n",
    "\n",
    "\n",
    "scaling_rows = []\n",
    "for label, series in all_series.items():\n",
    "    hurst = hurst_rs(series)\n",
    "    alpha = dfa_alpha(series)\n",
    "    scaling_rows.append({\"Regime\": label, \"H\": hurst, \"alpha_DFA\": alpha})\n",
    "\n",
    "scaling_df = pd.DataFrame(scaling_rows).set_index(\"Regime\")\n",
    "scaling_df\n"
   ]
  },
}